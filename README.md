# nemoGazeTracking


아래의 그림은 본 프로젝트에서 개발한 기술의 구조를 나타낸다. 본 프로젝트에서 개발한 기술은 Python 파일을 통해 실행되고 크게 시선 인식 기능과 인식 결과를 화면에 송출하는 기능으로 나뉜다. 시선 인식의 경우, 우선적으로 사람들의 얼굴을 인식해 눈과 동공의 위치를 계산해 사람들이 어디를 보고 있는 지 확인한다. 그리고 가장 많은 사람들이 바라보고 있는 방향을 모니터에 출력한다. 시선 인식 기술은 우선적으로 실시간으로 받는 이미지에서 사람의 얼굴을 인식한다. 그 후, 기계학습 및 이미지 처리 기능이 있는 dlib 라이브러리를 사용해 얼굴 인식을 수행한다. 얼굴 인식을 위해서 dlib의 face detector 클래스를 사용해 인식한 얼굴들의 정보를 리턴한다. 인식된 얼굴들의 데이터는 faces라는 리스트 자료구조로 저장되어 시선추적을 위해 사용된다. 확인된 얼굴들의 데이터를 사용해 각 얼굴별로 GazeTracking 클래스의 인스턴스를 생성하고 시선을 추적한다. GazeTracking 클래스에서는 얼굴의 데이터를 사용해 attribute로서 Eye 클래스로 눈과 동공의 위치 정보를 추출해낸다. 이때 동공 감지 알고리즘을 교정하여 웹캠에서의 사람 인식률 상승을 위해 Calibration 클래스가 사용된다. Eye 클래스로 추출된 눈의 정보는 왼쪽과 오른쪽 눈의 정보를 별개로 저장하며 각 눈의 위치와 눈동자 속 동공의 위치를 확인해 이 위치의 비율을 도출해낸다. 눈과 동공의 위치 정보는 좌표로 추출되며 이를 사용해 현재 사용자의 시선을 계산해 낸다. 사용자의 시선은 가로 및 세로 비율로 0과 1사이의 값으로 제공된다. 이 비율을 사용하여 현재 감지된 사람이 상하좌우 어디를 보고 있는 지 확인 할 수 있고 현재는 좌우 판단을 위해 0~0.4를 오른쪽으로 0.6-1.0을 오른쪽으로 판별하고 있다. 또한 얼굴 및 동공 감지를 위해서 두 가지 머신러닝 학습 데이터가 사용되고 있는데 이를 개선하면 시선 인식률을 더욱 높일 수 있다.
웹캠으로 감지한 모든 얼굴의 시선을 확인한 후, 가장 많은 사람들이 보고 있는 모니터의 방향을 붉은 사각형으로 Highlight하게 구현하였다. 그리고 해당 Highlight기능은 추후 코드 몇 줄만으로 세분화되기 쉽게 개발하였다. 따라서 추후 4분할 혹은 9분할로 나누어 사람들의 시선을 좀 더 세밀하게 구분하거나 한 방향을 바라보고 있는 사람들의 수를 더 쉽게 확인할 수 있다.
